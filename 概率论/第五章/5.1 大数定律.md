# 随机变量收敛
设 $Y_1, Y_2, \dots, Y_n, \dots$ 为随机变量序列，若存在常数 $a$，对任意给定的 $\varepsilon > 0$，有 $$ \lim_{n\to\infty} P(|Y_n - a| \geq \varepsilon) = 0, \quad \text{或} \quad \lim_{n\to\infty} P(|Y_n - a| < \varepsilon) = 1. $$则称随机变量序列 $Y_1, Y_2, \dots, Y_n, \dots$ **依概率收敛**于 $a$，记为 $Y_n \xrightarrow{P} a$。
> 随机变量序列的收敛性是指 $n \to \infty$ 时，随机变量 $Y_n$ 与常数 $a$ 的偏差小于任意正数 $\varepsilon$ 的概率将趋向于 $1$，这与数学分析中的极限定义有明显不同。

# 大数定律
1. 设 $X_1, X_2, \dots, X_n, \dots$ 为随机变量序列，若对任意给定的 $\varepsilon > 0$，有 $$ \lim_{n\to\infty} P\left(\left| \frac{1}{n}\sum_{k=1}^n X_k - \frac{1}{n}\sum_{k=1}^n EX_k \right| \geq \varepsilon\right) = 0 $$或 $$ \lim_{n\to\infty} P\left(\left| \frac{1}{n}\sum_{k=1}^n X_k - \frac{1}{n}\sum_{k=1}^n EX_k \right| < \varepsilon\right) = 1, $$ 则称 $\{X_n\}$ 服从**大数定律**。
2. **切比雪夫大数定律**：设 $X_1, X_2, \dots$ 为两两不相关的随机变量序列，其方差一致有界，即存在常数 $C$，使得 $DX_k < C$，对一切 $k=1, 2, \dots$ 成立，则 $\{X_n\}$ 服从**大数定律**，即 $$ \lim_{n\to\infty} P\left(\left| \frac{1}{n}\sum_{k=1}^n X_k - \frac{1}{n}\sum_{k=1}^n EX_k \right| \geq \varepsilon\right) = 0 $$或 $$ \frac{1}{n}\sum_{k=1}^n X_k \xrightarrow{P} \frac{1}{n}\sum_{k=1}^n EX_k $$
	> *证明*：先证 $\frac{1}{n^2} D\left(\sum_{k=1}^n X_k\right) \to 0$。由于 $\{X_k\}$ 两两互不相关，即 $Cov(X_i, X_j) = 0$，所以 $$ \frac{1}{n^2} D\left(\sum_{k=1}^n X_k\right) = \frac{1}{n^2}\left[\sum_{k=1}^n DX_k + 2 \sum_{1\leq i < j \leq n} Cov(X_i, X_j)\right] $$ $$ = \frac{1}{n^2} \sum_{k=1}^n DX_k < \frac{1}{n^2} \sum_{k=1}^n C = \frac{C}{n} \to 0 $$根据切比雪夫不等式 $$ 0 \leq P\left(\left| \frac{1}{n}\sum_{k=1}^n X_k - \frac{1}{n}\sum_{k=1}^n EX_k \right| \geq \varepsilon\right) = P\left(\left| \frac{1}{n}\sum_{k=1}^n X_k - E\left(\frac{1}{n}\sum_{k=1}^n X_k\right) \right| \geq \varepsilon\right) $$ $$ \leq \frac{D\left(\frac{1}{n}\sum_{k=1}^n X_k\right)}{\varepsilon^2} = \frac{\frac{1}{n^2}D\left(\sum_{k=1}^n X_k\right)}{\varepsilon^2} \to 0 $$
3. **独立同分布大数定律**：设 $X_1, X_2, \dots$ 为**相互独立且分布相同**的随机变量序列，数学期望存在， $EX_n = \mu$，则 $\{X_n\}$ 服从**大数定律**，即 $$ \lim_{n\to\infty} P\left(\left| \frac{1}{n}\sum_{k=1}^n X_k - \mu \right| \geq \varepsilon\right) = 0 $$
	> *证明*：只在方差存在的情况下给出证明。 
		因为 $\{X_n\}$ 同分布，设 $DX_n = \sigma^2 < \infty$，从而方差一致有界，由于 $X_1, X_2, \dots$ 相互独立，必有两两互不相关，而 $\frac{1}{n}\sum_{k=1}^n EX_k = \mu$，根据切比雪夫大数定律 $$ \lim_{n\to\infty} P\left(\left| \frac{1}{n}\sum_{i=1}^n X_i - \frac{1}{n}\sum_{i=1}^n EX_i \right| \geq \varepsilon\right) = \lim_{n\to\infty} P\left(\left| \frac{1}{n}\sum_{i=1}^n X_i - \mu \right| \geq \varepsilon\right) = 0, $$ 因此，大数定律成立。可以表示为 $\frac{1}{n}\sum_{k=1}^n X_k \xrightarrow{P} \mu$。
		
4. 伯努利（辛钦）大数定律：设 $n_A$ 是 $n$ 重伯努利试验中 $A$ 发生的次数，$A$ 发生的概率为 $p$，则对任意 $\varepsilon > 0$，有 $$ \lim_{n\to\infty} P\left(\left| \frac{n_A}{n} - p \right| \geq \varepsilon\right) = 0 $$
	>*证明*：令 $$ X_k = \begin{cases} 1, & \text{第 } k \text{ 次试验中 } A \text{ 发生} \\ 0, & \text{第 } k \text{ 次试验中 } A \text{ 不发生} \end{cases}, \quad k = 1, 2, \dots, n. $$ 则 $X_1, X_2, \dots, X_n$ 相互独立，均服从 $0-1$ 分布，$EX_k = p$，$\frac{1}{n}\sum_{k=1}^n EX_k = p$。 显然， $n_A = X_1 + X_2 + \dots + X_n$，$\frac{n_A}{n} = \frac{1}{n}\sum_{k=1}^n X_k$，由独立同分布大数定律 $$ \lim_{n\to\infty} P\left(\left| \frac{n_A}{n} - p \right| \geq \varepsilon\right) = \lim_{n\to\infty} P\left(\left| \frac{1}{n}\sum_{k=1}^n X_k - \frac{1}{n}\sum_{k=1}^n EX_k \right| \geq \varepsilon\right) = 0 $$或 $$ \lim_{n\to\infty} P\left(\left| \frac{n_A}{n} - p \right| < \varepsilon\right) = 1. $$